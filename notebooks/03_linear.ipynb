{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf03473e",
   "metadata": {},
   "source": [
    "# Day 4\n",
    "\n",
    "## 线性回归手动实现\n",
    "\n",
    "### 使用PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada92b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 91479.0000\n",
      "Epoch 100: Loss = 17.1703\n",
      "Epoch 200: Loss = 17.1666\n",
      "Epoch 300: Loss = 17.1629\n",
      "Epoch 400: Loss = 17.1592\n",
      "Epoch 500: Loss = 17.1555\n",
      "Epoch 600: Loss = 17.1519\n",
      "Epoch 700: Loss = 17.1482\n",
      "Epoch 800: Loss = 17.1445\n",
      "Epoch 900: Loss = 17.1408\n",
      "Epoch 1000: Loss = 17.1371\n",
      "Epoch 1100: Loss = 17.1334\n",
      "Epoch 1200: Loss = 17.1297\n",
      "Epoch 1300: Loss = 17.1260\n",
      "Epoch 1400: Loss = 17.1223\n",
      "Epoch 1500: Loss = 17.1186\n",
      "Epoch 1600: Loss = 17.1149\n",
      "Epoch 1700: Loss = 17.1112\n",
      "Epoch 1800: Loss = 17.1075\n",
      "Epoch 1900: Loss = 17.1038\n",
      "Epoch 2000: Loss = 17.1002\n",
      "Epoch 2100: Loss = 17.0965\n",
      "Epoch 2200: Loss = 17.0928\n",
      "Epoch 2300: Loss = 17.0891\n",
      "Epoch 2400: Loss = 17.0854\n",
      "Epoch 2500: Loss = 17.0817\n",
      "Epoch 2600: Loss = 17.0780\n",
      "Epoch 2700: Loss = 17.0744\n",
      "Epoch 2800: Loss = 17.0707\n",
      "Epoch 2900: Loss = 17.0670\n",
      "Epoch 3000: Loss = 17.0634\n",
      "Epoch 3100: Loss = 17.0597\n",
      "Epoch 3200: Loss = 17.0561\n",
      "Epoch 3300: Loss = 17.0524\n",
      "Epoch 3400: Loss = 17.0487\n",
      "Epoch 3500: Loss = 17.0451\n",
      "Epoch 3600: Loss = 17.0415\n",
      "Epoch 3700: Loss = 17.0378\n",
      "Epoch 3800: Loss = 17.0342\n",
      "Epoch 3900: Loss = 17.0306\n",
      "Epoch 4000: Loss = 17.0269\n",
      "Epoch 4100: Loss = 17.0233\n",
      "Epoch 4200: Loss = 17.0197\n",
      "Epoch 4300: Loss = 17.0160\n",
      "Epoch 4400: Loss = 17.0124\n",
      "Epoch 4500: Loss = 17.0088\n",
      "Epoch 4600: Loss = 17.0052\n",
      "Epoch 4700: Loss = 17.0015\n",
      "Epoch 4800: Loss = 16.9979\n",
      "Epoch 4900: Loss = 16.9943\n",
      "Epoch 5000: Loss = 16.9907\n",
      "Epoch 5100: Loss = 16.9870\n",
      "Epoch 5200: Loss = 16.9834\n",
      "Epoch 5300: Loss = 16.9798\n",
      "Epoch 5400: Loss = 16.9762\n",
      "Epoch 5500: Loss = 16.9725\n",
      "Epoch 5600: Loss = 16.9689\n",
      "Epoch 5700: Loss = 16.9653\n",
      "Epoch 5800: Loss = 16.9617\n",
      "Epoch 5900: Loss = 16.9580\n",
      "Epoch 6000: Loss = 16.9544\n",
      "Epoch 6100: Loss = 16.9508\n",
      "Epoch 6200: Loss = 16.9472\n",
      "Epoch 6300: Loss = 16.9435\n",
      "Epoch 6400: Loss = 16.9399\n",
      "Epoch 6500: Loss = 16.9363\n",
      "Epoch 6600: Loss = 16.9327\n",
      "Epoch 6700: Loss = 16.9290\n",
      "Epoch 6800: Loss = 16.9254\n",
      "Epoch 6900: Loss = 16.9218\n",
      "Epoch 7000: Loss = 16.9182\n",
      "Epoch 7100: Loss = 16.9146\n",
      "Epoch 7200: Loss = 16.9110\n",
      "Epoch 7300: Loss = 16.9073\n",
      "Epoch 7400: Loss = 16.9037\n",
      "Epoch 7500: Loss = 16.9001\n",
      "Epoch 7600: Loss = 16.8965\n",
      "Epoch 7700: Loss = 16.8929\n",
      "Epoch 7800: Loss = 16.8893\n",
      "Epoch 7900: Loss = 16.8856\n",
      "Epoch 8000: Loss = 16.8820\n",
      "Epoch 8100: Loss = 16.8784\n",
      "Epoch 8200: Loss = 16.8748\n",
      "Epoch 8300: Loss = 16.8712\n",
      "Epoch 8400: Loss = 16.8676\n",
      "Epoch 8500: Loss = 16.8640\n",
      "Epoch 8600: Loss = 16.8604\n",
      "Epoch 8700: Loss = 16.8567\n",
      "Epoch 8800: Loss = 16.8531\n",
      "Epoch 8900: Loss = 16.8495\n",
      "Epoch 9000: Loss = 16.8459\n",
      "Epoch 9100: Loss = 16.8423\n",
      "Epoch 9200: Loss = 16.8387\n",
      "Epoch 9300: Loss = 16.8351\n",
      "Epoch 9400: Loss = 16.8315\n",
      "Epoch 9500: Loss = 16.8279\n",
      "Epoch 9600: Loss = 16.8243\n",
      "Epoch 9700: Loss = 16.8206\n",
      "Epoch 9800: Loss = 16.8170\n",
      "Epoch 9900: Loss = 16.8134\n",
      "Learned wights: [[0.69388074]\n",
      " [0.08876424]]\n",
      "Learned bias: [0.7138583]\n",
      "Predicted y for new_X: 47.87150573730469\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 特征\n",
    "X = torch.tensor([\n",
    "    [3.0, 200.0],\n",
    "    [5.0, 400.0],\n",
    "    [2.0, 150.0],\n",
    "    [8.0, 800.0],\n",
    "    [6.0, 600.0]\n",
    "])\n",
    "\n",
    "# 标签\n",
    "y = torch.tensor([25.0, 45.0, 20.0, 75.0, 55.0]).view(-1, 1)\n",
    "\n",
    "# 权重\n",
    "w = torch.randn((2, 1), requires_grad=True) # 随机\n",
    "b = torch.randn((1, ), requires_grad=True)\n",
    "\n",
    "# 预测函数\n",
    "def predict(X):\n",
    "    return X @ w + b\n",
    "\n",
    "# 损失函数\n",
    "def mse_loss(y_pred, y):\n",
    "    return torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "# 用自动求导计算梯度，然后手动更新参数\n",
    "lr = 1e-6\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播\n",
    "    y_pred = predict(X)\n",
    "    loss = mse_loss(y_pred, y)\n",
    "\n",
    "    # 反向传播，自动算梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 手动更新参数，梯度下降\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # 梯度清零，否则会累积\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # 每100轮打印一次损失\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# 查看训练结果\n",
    "print(\"Learned wights:\", w.data.numpy())\n",
    "print(\"Learned bias:\", b.data.numpy())\n",
    "\n",
    "# 预测一个新数据\n",
    "new_X = torch.tensor([[4.0, 500.0]])\n",
    "predicted_y = predict(new_X)\n",
    "print(\"Predicted y for new_X:\", predicted_y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb26d24",
   "metadata": {},
   "source": [
    "### 只使用Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1c8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [3.0, 200.0],\n",
    "    [5.0, 400.0],\n",
    "    [2.0, 150.0],\n",
    "    [8.0, 800.0],\n",
    "    [6.0, 600.0]\n",
    "])\n",
    "y = np.array([[25.0], [45.0], [20.0], [75.0], [55.0]])\n",
    "\n",
    "n, d = X.shape\n",
    "\n",
    "# 标准化\n",
    "X_mean = X.mean(axis=0, keepdims=True)\n",
    "X_std = X.std(axis=0, keepdims=True)\n",
    "Xn = (X - X_mean) / X_std\n",
    "\n",
    "# 初始化参数\n",
    "# 权重 w (d,1)，偏置 b (标量)\n",
    "rng = np.random.default_rng(42)\n",
    "w = rng.normal(0, 0.1, size=(d, 1))  # d=2\n",
    "b = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef21bf",
   "metadata": {},
   "source": [
    "#### 初始化参数\n",
    "\n",
    "- default_rng(42)：创建随机数生成器，固定随机种子。\n",
    "- rng.normal(0, 0.1, size=(d,1))：从均值=0，标准差=0.1 的正态分布里采样，生成权重矩阵。\n",
    "    - 均值 0：保证初始化时无偏。\n",
    "    - 标准差 0.1：让初始值比较小，避免梯度过大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f5fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss: 2342.941688\n",
      "Epoch  100 | Loss: 1.248470\n",
      "Epoch  200 | Loss: 1.218594\n",
      "Epoch  300 | Loss: 1.198741\n",
      "Epoch  400 | Loss: 1.185549\n",
      "Epoch  500 | Loss: 1.176783\n",
      "Epoch  600 | Loss: 1.170958\n",
      "Epoch  700 | Loss: 1.167087\n",
      "Epoch  800 | Loss: 1.164515\n",
      "Epoch  900 | Loss: 1.162806\n",
      "\n",
      "Learned weights:\n",
      " [[12.3567521 ]\n",
      " [ 7.76202692]]\n",
      "Learned bias: 43.999999999999986\n",
      "Prediction for x_new: 35.23745137848098\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lr = 0.1       # 学习率\n",
    "epochs = 1000  # 迭代次数\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播：预测值\n",
    "    y_hat = Xn @ w + b   # (n,1)\n",
    "\n",
    "    # 计算误差\n",
    "    diff = y_hat - y     # (n,1)\n",
    "\n",
    "    # 均方误差 (MSE)\n",
    "    loss = np.mean(diff ** 2)\n",
    "\n",
    "    # 反向传播（手写梯度）\n",
    "    grad_w = (2 / n) * (Xn.T @ diff)  # (d,1)\n",
    "    grad_b = (2 / n) * np.sum(diff)   # 标量\n",
    "\n",
    "    # 参数更新\n",
    "    w -= lr * grad_w\n",
    "    b -= lr * grad_b\n",
    "\n",
    "    # 每 100 次打印一次损失\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | Loss: {loss:.6f}\")\n",
    "\n",
    "print(\"\\nLearned weights:\\n\", w)\n",
    "print(\"Learned bias:\", b)\n",
    "\n",
    "# 测试预测\n",
    "x_new = np.array([[4.0, 300.0]])\n",
    "x_new_n = (x_new - X_mean) / X_std\n",
    "y_pred = x_new_n @ w + b\n",
    "print(\"Prediction for x_new:\", float(y_pred))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
